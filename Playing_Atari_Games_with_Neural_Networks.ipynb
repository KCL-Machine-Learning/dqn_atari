{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Playing Atari Games with Neural Networks.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOLXTdbGazTh3q6eSql5rJz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCL-Machine-Learning/dqn_atari/blob/main/Playing_Atari_Games_with_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1Pf2WT4nAqq"
      },
      "source": [
        "#@title\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
        "!pip install unrar\n",
        "!unrar x Roms.rar\n",
        "!mkdir rars\n",
        "!mv HC\\ ROMS.zip   rars\n",
        "!mv ROMS.zip  rars\n",
        "!python -m atari_py.import_roms rars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTD_JdcbnNP2"
      },
      "source": [
        "# Playing Atrai games with Neural Network\n",
        "\n",
        "### Papers: \n",
        "- [Playing Atari with Deep Reinforcment Learning](https://arxiv.org/pdf/1312.5602.pdf)\n",
        "- [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLn3lpxcfuBQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt4u8-60gW2I"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/DQN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utMpKX4Sme8X"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EdoORJJauXQ"
      },
      "source": [
        "import random\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import deque, namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igkRyVhKnioq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display\n",
        "\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.axis('off')\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7pSupQ1mvSZ"
      },
      "source": [
        "\n",
        "env = gym.make('Breakout-v0')\n",
        "for i_episode in range(1):\n",
        "    observation = env.reset()\n",
        "    for t in range(200):\n",
        "        show_state(env)\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        print(reward, done, info)\n",
        "        \n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsSUYtG9VDlr"
      },
      "source": [
        "observation, observation.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AHteTsEWaXO"
      },
      "source": [
        "ACTION_SIZE = env.action_space.n - 1\n",
        "STACK = 4\n",
        "INPUT_DIM = (64, 64, STACK)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTYcNnFRc4yS"
      },
      "source": [
        "ACTION_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy95ewFBVfbq"
      },
      "source": [
        "processed_obs = tf.cast(observation, tf.float32)\n",
        "processed_obs /= 255.0\n",
        "processed_obs = tf.image.rgb_to_yuv(processed_obs)[:, :, :1]\n",
        "processed_obs = tf.image.resize(processed_obs, (64, 64))\n",
        "plt.imshow(processed_obs[..., 0])\n",
        "processed_obs, processed_obs.numpy().max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noUc1G6InSZU"
      },
      "source": [
        "env.action_space.sample()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwFigZq6o_Ys"
      },
      "source": [
        "env.unwrapped.get_action_meanings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZN02CEBVvPn"
      },
      "source": [
        "BUFFER_SIZE = int(1e6)         # replay buffer size\n",
        "BATCH_SIZE = 32                # minibatch size\n",
        "GAMMA = 0.99                   # discount factor\n",
        "PARAM_UPDATE_EVERY = 1         # how often to update the parameters\n",
        "TARGET_UPDATE_EVERY = 10000    # how often to update the target network\n",
        "START_LEARNING = 15000         # start learning after how many steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjmDxAi9pZWl"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size=BUFFER_SIZE, input_shape=INPUT_DIM[:2], history_length=STACK):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            size: Integer, Number of stored transitions\n",
        "            input_shape: Shape of the preprocessed frame\n",
        "            history_length: Integer, Number of frames stacked together to create a state for the agent\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.input_shape = input_shape\n",
        "        self.history_length = history_length\n",
        "        self.count = 0  # total index of memory written to, always less than self.size\n",
        "        self.current = 0  # index to write to\n",
        "\n",
        "        # Pre-allocate memory\n",
        "        self.actions = np.empty(self.size, dtype=np.int32)\n",
        "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
        "        self.frames = np.empty((self.size, self.input_shape[0], self.input_shape[1], 1), dtype=np.float32)\n",
        "        self.next_frames = np.empty((self.size, self.input_shape[0], self.input_shape[1], 1), dtype=np.float32)\n",
        "        self.dones = np.empty(self.size, dtype=np.bool)\n",
        "\n",
        "\n",
        "    def add(self, frame, action, reward, next_frame, done):\n",
        "        \"\"\"Saves a transition to the replay buffer\n",
        "        Arguments:\n",
        "            action: An integer between 0 and env.action_space.n - 1 \n",
        "                determining the action the agent perfomed\n",
        "            frame: A (84, 84, 1) frame of the game in grayscale\n",
        "            reward: A float determining the reward the agend received for performing an action\n",
        "            done: A bool stating whether the episode terminated\n",
        "        \"\"\"\n",
        "\n",
        "        self.actions[self.current] = action\n",
        "        self.frames[self.current, ...] = frame\n",
        "        self.next_frames[self.current, ...] = next_frame\n",
        "        self.rewards[self.current] = reward\n",
        "        self.dones[self.current] = done\n",
        "        self.count = max(self.count, self.current+1)\n",
        "        self.current = (self.current + 1) % self.size\n",
        "\n",
        "    def sample(self, batch_size=BATCH_SIZE):\n",
        "        \"\"\"Returns a minibatch of self.batch_size = 32 transitions\n",
        "        Arguments:\n",
        "            batch_size: How many samples to return\n",
        "        Returns:\n",
        "            A tuple of states, actions, rewards, new_states, and dones\n",
        "        \"\"\"\n",
        "\n",
        "        # Get a list of valid indices\n",
        "        indices = random.sample(range(0, self.count), batch_size)\n",
        "        states = np.zeros((batch_size, self.input_shape[0], self.input_shape[1], self.history_length))\n",
        "        next_states = np.zeros((batch_size, self.input_shape[0], self.input_shape[1], self.history_length))\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            for j in range(self.history_length): # 0, 1, 2, 3   \n",
        "                if j > 0 and self.dones[idx-j]:\n",
        "                    break\n",
        "                states[i, :, :, j] =  self.frames[idx-j, :, :, 0]\n",
        "                next_states[i, :, :, j] = self.next_frames[idx-j, :, :, 0]\n",
        "\n",
        "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "        actions = tf.convert_to_tensor(self.actions[indices], dtype=tf.int32)\n",
        "        rewards = tf.convert_to_tensor(self.rewards[indices], dtype=tf.float32)\n",
        "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
        "        dones = tf.convert_to_tensor(self.dones[indices], dtype=tf.float32)\n",
        "        return states,  actions, rewards, next_states, dones\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"Save the replay buffer to a folder\"\"\"\n",
        "\n",
        "        np.save('actions.npy', self.actions)\n",
        "        np.save('frames.npy', self.frames)\n",
        "        np.save('next_frames.npy', self.next_frames)\n",
        "        np.save('rewards.npy', self.rewards)\n",
        "        np.save('dones.npy', self.dones)\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"Loads the replay buffer from a folder\"\"\"\n",
        "        self.actions = np.load('actions.npy')\n",
        "        self.frames = np.load('frames.npy')\n",
        "        self.next_frames = np.load('next_frames.npy')\n",
        "        self.rewards = np.load('rewards.npy')\n",
        "        self.dones = np.load('dones.npy')\n",
        "    \n",
        "    def __len__(self):\n",
        "\n",
        "        return self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYn5jG8Oq2jE"
      },
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, input_dim, action_size=9, seed=123):\n",
        "        tf.random.set_seed(seed)\n",
        "        self.local_model = self.__build_model__(input_dim, action_size)\n",
        "        self.local_model.compile(optimizer=tf.keras.optimizers.RMSprop(\n",
        "            learning_rate=0.00025, rho=0.95, momentum=0.95))\n",
        "        self.target_model = self.__build_model__(input_dim, action_size)\n",
        "        self.target_model.set_weights(self.local_model.get_weights())\n",
        "\n",
        "        self.action_size = action_size\n",
        "        self.input_dim = input_dim\n",
        "        self.start_learning = False\n",
        "        self.memory = ReplayBuffer()\n",
        "        self.t_step = 0\n",
        "\n",
        "    def __build_model__(self, input_dim, action_size):\n",
        "        input_layer = tf.keras.layers.Input(shape=input_dim)\n",
        "        x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), strides=4, kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.), activation=\"relu\")(input_layer)\n",
        "        x = tf.keras.layers.Flatten()(x)\n",
        "        x = tf.keras.layers.Dense(512, activation=\"relu\",kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.))(x)\n",
        "        output = tf.keras.layers.Dense(action_size,kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.))(x)\n",
        "    \n",
        "        return tf.keras.Model(inputs=input_layer, outputs=output)\n",
        "    \n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        self.t_step = self.t_step + 1 \n",
        "        \n",
        "        if not self.start_learning and self.t_step % START_LEARNING == 0:\n",
        "            self.start_learning = True\n",
        "            print(\"Starting learning from\", self.t_step)\n",
        "            \n",
        "\n",
        "        # If enough samples are available in memory, get random subset and learn\n",
        "        if len(self.memory) > BATCH_SIZE and self.start_learning and self.t_step % PARAM_UPDATE_EVERY == 0:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "        \n",
        "        if self.start_learning and self.t_step % TARGET_UPDATE_EVERY == 0:\n",
        "            self.update_target_network()\n",
        "            self.t_step = 0\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "        action_values = self.local_model.predict(state)\n",
        "        if random.random() > eps:\n",
        "            action = np.argmax(action_values)\n",
        "        else:\n",
        "            action = random.choice(np.arange(self.action_size))\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        # Get max predicted Q values (for next states) from target model\n",
        "        target_q_next = tf.reduce_max(self.target_model.predict(next_states), axis=1)\n",
        "        target_q = rewards + (gamma * target_q_next * (1-dones))\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.local_model(states, training=True)\n",
        "            one_hot_actions = tf.keras.utils.to_categorical(actions, self.action_size, dtype=np.float32)\n",
        "            predicted_q = tf.reduce_sum(tf.multiply(q_values, one_hot_actions), axis=1)\n",
        "            loss = tf.keras.losses.Huber()(target_q, predicted_q)\n",
        "        \n",
        "        model_gradients = tape.gradient(loss, self.local_model.trainable_weights)\n",
        "\n",
        "        self.local_model.optimizer.apply_gradients(zip(model_gradients, self.local_model.trainable_weights))\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_model.set_weights(self.local_model.get_weights())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoCr5dQ0pm6d"
      },
      "source": [
        "def dqn(agent, n_episodes=2000, max_t=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.9969, train=True, window_size=10):\n",
        "    \"\"\"Deep Q-Learning.\n",
        "    \n",
        "    Params\n",
        "    ======\n",
        "        n_episodes (int): maximum number of training episodes\n",
        "        max_t (int): maximum number of timesteps per episode\n",
        "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "        eps_end (float): minimum value of epsilon\n",
        "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "        train (bool): to update agent or not\n",
        "    \"\"\"\n",
        "    max_score = -100\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=window_size)  # last 100 scores\n",
        "    eps = eps_start                    # initialize epsilon\n",
        "    print(\"Starting training\")\n",
        "    \n",
        "    env = gym.make('Breakout-v0')\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        frame = env.reset()\n",
        "        frame = tf.cast(frame, tf.float32)\n",
        "        frame /= 255\n",
        "        frame = tf.image.rgb_to_yuv(frame)[:, :, :1]\n",
        "        frame = tf.image.resize(frame, size=tf.convert_to_tensor(INPUT_DIM, dtype=tf.int32)[:2])\n",
        "        \n",
        "        stack = np.zeros(INPUT_DIM)\n",
        "        stack[:, :, 0] = frame[:, :, 0]\n",
        "        \n",
        "        score = 0\n",
        "        lives = 5\n",
        "        for t in range(max_t):\n",
        "            state = tf.expand_dims(stack, axis=0)\n",
        "            action = int(agent.act(state, eps))\n",
        "            next_frame, reward, done, info = env.step(action) \n",
        "\n",
        "            if info['ale.lives'] < lives:\n",
        "                reward = -1.0\n",
        "                lives = info['ale.lives']\n",
        "                done = True\n",
        "            else:\n",
        "                reward -= 0.5\n",
        "\n",
        "            next_frame = tf.cast(next_frame, tf.float32)\n",
        "            next_frame /= 255.0\n",
        "            next_frame = tf.image.rgb_to_yuv(next_frame)[:, :, :1]\n",
        "            next_frame = tf.image.resize(next_frame, size=tf.convert_to_tensor(INPUT_DIM, dtype=tf.int32)[:2])\n",
        "            \n",
        "            if train:\n",
        "                agent.step(frame, action, reward, next_frame, done)\n",
        "\n",
        "            for i in range(STACK-1):\n",
        "                stack[:, :, STACK-(i+1)] = stack[:, :, STACK-(i+2)]\n",
        "            stack[:, :, 0] = next_frame[:, :, 0]\n",
        "            frame = next_frame\n",
        "\n",
        "            score += reward\n",
        "            if done and lives == 0:\n",
        "                break\n",
        "                \n",
        "        scores_window.append(score)       # save most recent score\n",
        "        scores.append(score)              # save most recent score\n",
        "        agent.local_model.save_weights('breakout_local_model_checkpoint')\n",
        "        agent.target_model.save_weights('breakout_target_model_checkpoint')\n",
        "        print('\\rEpisode {}\\tT_Step: {}\\tAverage Score: {:.2f}\\tMax score: {:.2f}\\tMin score: {:.2f}\\teps: {:.2f}'.format(i_episode, agent.t_step, np.mean(scores_window), np.max(scores_window), np.min(scores_window), eps), end=\"\")\n",
        "        if train:\n",
        "            eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "            if i_episode % window_size == 0:\n",
        "                eval_score = evaluate(agent)\n",
        "                agent.memory.save()\n",
        "                print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMax score: {:.2f}\\tMin score: {:.2f}\\teps: {:.2f}\\tEval Score: {:.2f}'.format(i_episode, np.mean(scores_window), np.max(scores_window), np.min(scores_window), eps, eval_score))\n",
        "    env.close()\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwWn4YNPWOKo"
      },
      "source": [
        "agent = Agent(INPUT_DIM, ACTION_SIZE)\n",
        "# agent.local_model.load_weights('breakout_local_model_checkpoint')\n",
        "# agent.target_model.load_weights('breakout_target_model_checkpoint')\n",
        "# agent.memory.load()\n",
        "scores = dqn(agent, n_episodes=1000, max_t=800, eps_start=1.0, eps_end=0.1, eps_decay=0.99, train=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXlf97dPXzsK"
      },
      "source": [
        "import time\n",
        "def evaluate(agent, render=False):\n",
        "    \n",
        "    env = gym.make('Breakout-v0')\n",
        "    state = env.reset()\n",
        "    state = tf.cast(state, tf.float32)\n",
        "    state /= 255\n",
        "    state = tf.image.rgb_to_yuv(state)[:, :, :1]\n",
        "    state = tf.image.resize(state, size=tf.convert_to_tensor(INPUT_DIM, dtype=tf.int32)[:2])\n",
        "    stack = np.zeros(INPUT_DIM)\n",
        "    stack[:, :, 0] = state[:, :, 0]\n",
        "    state = tf.expand_dims(stack, axis=0)\n",
        "    env.step(1)\n",
        "    score = 0\n",
        "    for t in range(200):\n",
        "        if render:\n",
        "            show_state(env)\n",
        "        action = int(agent.act(state, 0.))\n",
        "        # print(action)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        state = tf.cast(state, tf.float32)\n",
        "        state /= 255\n",
        "        state = tf.image.rgb_to_yuv(state)[:, :, :1]\n",
        "        state = tf.image.resize(state, size=tf.convert_to_tensor(INPUT_DIM, dtype=tf.int32)[:2])\n",
        "        for i in range(STACK-1):\n",
        "            stack[:, :, STACK-(i+1)] = stack[:, :, STACK-(i+2)]\n",
        "        stack[:, :, 0] = state[:, :, 0]\n",
        "        \n",
        "        state = tf.expand_dims(stack, axis=0)\n",
        "        score += reward\n",
        "        if render:\n",
        "            time.sleep(1)\n",
        "        if done:\n",
        "            break\n",
        "    env.close()\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsRLWu2kXnrU"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z-uUnt2V83U"
      },
      "source": [
        "evaluate(agent, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aixJ1zn7XRDF"
      },
      "source": [
        "agent = Agent(INPUT_DIM, ACTION_SIZE)\n",
        "agent.local_model.load_weights('breakout_local_model_checkpoint')\n",
        "agent.target_model.load_weights('breakout_target_model_checkpoint')\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmbRH5a3Xy9I"
      },
      "source": [
        "plt.imshow(tf.image.rgb_to_yuv(state)[0, :, :, 0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMrC4QPSAg-L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}